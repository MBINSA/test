{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d410e39-e0d4-4430-ba0f-fc8c59d620d0",
   "metadata": {},
   "source": [
    "# Code Python - Electre Tri "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c038ddb9",
   "metadata": {},
   "source": [
    "## Introduction to the project\n",
    "\n",
    "The company at the origin of the request is the social landlord 3F, part of the national group Action Logement. Its request concerns the renovation of three of its housing buildings located in the Lyon region. These buildings having been built in the years 2014, the company thus considered necessary to carry out renovation works for the whole of these 3 buildings. \n",
    "\n",
    "The company has found it difficult in the past to find out how to renovate a building in view regarding the different aspects that come into play. For example, when looking at the minimum energy loss between primary and final energy, gas appears to be the most interesting form of energy. However, when looking at the environmental impact of this form of energy, gas is badly ranked. Thus, they wish to take into account several aspects of energy renovation in this project.  \n",
    "\n",
    "Since several options of renovation are possible and the decision is based on multiple criteria, it has been chosen thaht a multi-criteria analysis should be carried out.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c329d900",
   "metadata": {},
   "source": [
    "### Electre Tri as a multi-criteria analysis \n",
    "\n",
    "The Electre Tri method is the multi-criteria analysis that is selected for the project. In its process, the input data for each item is normalised using thresholds and compared to difference profiles that separate categories. This method results in an optimistic and pessimistic ranking of the elements in which every actions are ranked in categories.\n",
    "\n",
    "The following code allows to execute step by step the calculations of the Electre Tri method :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bc9ebed",
   "metadata": {},
   "source": [
    "*expliquer peut être comment fonctionne la méthode vite fait en mode il y a 28 scenarios 16 critères et il faut poids thresholds etc*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcdcf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random, vstack, empty\n",
    "import math\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f512a50",
   "metadata": {},
   "source": [
    "### Import of data from csv file as a Pandas Dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2d03a3f",
   "metadata": {},
   "source": [
    "The input of the whole analysis is a csv file containing the following informations : \n",
    "- The mean value of the performance of each scenario regarding each criteria \n",
    "- The weight of each criteria \n",
    "- The variance of each criteria\n",
    "- The 5 reference profiles : b0, b1, b2, b3, b4 and b5 \n",
    "- The 3 thresholds : q (the indiference threshold), p (the preference threshold), v (the veto threshold)\n",
    "\n",
    "It is imported as a dataframe 'd'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6003b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('Input_data.csv')\n",
    "λ = 0.75"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e703acc6",
   "metadata": {},
   "source": [
    "### Monte Carlo Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "126e492b",
   "metadata": {},
   "source": [
    "Explanation of what is Monte Carlo & how the function works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61789852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCarlo(d):\n",
    "    for i in d.index:\n",
    "        variance = d['VAR'][i]\n",
    "        for j in d.iloc[:, 0:28]:\n",
    "            m = d[j][i]\n",
    "            v = abs(m*variance)\n",
    "            perf = random.normal(m, v, 1)\n",
    "            d[j][i] = perf[0]\n",
    "    return d\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "268b5da4",
   "metadata": {},
   "source": [
    "autre version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73a5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCarlo(d):\n",
    "    variance = d['VAR'].values\n",
    "    m = d.iloc[:, 0:28].values\n",
    "    v = np.abs(m * variance[:, np.newaxis])\n",
    "    perf = np.random.normal(m, v)\n",
    "    d.iloc[:, 0:28] = perf\n",
    "    return d\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "590af63e",
   "metadata": {},
   "source": [
    "### Concordance\n",
    "\n",
    "The concordance matrix is a table that compare each pair of alternatives being considered, in our case, the sceanarios. In other words, it evaluates how well each option performs relative to the others with respect to the set of criteria. \n",
    "\n",
    "This function take as input the DataFrame containig all the performances as well as all the others parameters and input of the method, but only the performances, the reference profiles, and the thresholds will be used.\n",
    "\n",
    "The objective is to calculate the concordance between each pair of alternative and reference profiles and in both ways: \n",
    "- The concordance $C_j(a_i,b_k)$\n",
    "- The concordance $C_j(a_i,b_k)$ <br>\n",
    "*for $i$ the scenarios, $k$ the reference profiles and $j$ the criteria*\n",
    "\n",
    "Here is how the two types of concordance are calculated in the function: <br>\n",
    "<center>\n",
    "\n",
    "$C_j(a_i,b_k) = uj(a_i)-u_j(b_j)+p_j/p_j-q_j$<br>\n",
    "$C_j(b_k,a_i) = uj(b_j)-u_j(a_i)+p_j/p_j-q_j$<br>\n",
    "\n",
    "</center>\n",
    "\n",
    "\n",
    "If the value is higher than one it is replaced by one, and if it is smaller dans zero it is replaced by zero. \n",
    "\n",
    "Finally, the function returns two DataFrames : \n",
    "- `dconca` : The concordance between the performances and the reference profiles $C_k(a_i,b_j)$\n",
    "- `dconcb` : The concordance between the reference profiles and the performances $C_k(a_i,b_j)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conce2(d):\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df2 = pd.DataFrame()\n",
    "    for column in d.iloc[:, 0:28]: \n",
    "        for col in d.iloc[:,30:36]:\n",
    "            alpha = (d[column]-d[col]+d[d.columns[37]])/(d[d.columns[37]]-d[d.columns[36]])\n",
    "            beta = (d[col]-d[column]+d[d.columns[37]])/(d[d.columns[37]]-d[d.columns[36]])\n",
    "            new_df[column+col+' a,b']= alpha\n",
    "            new_df2[column+col+' b,a']= beta\n",
    "    new_df[new_df<0]=0\n",
    "    new_df[new_df>1]=1\n",
    "    new_df2[new_df2<0]=0\n",
    "    new_df2[new_df2>1]=1\n",
    "    return new_df, new_df2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b4c1f41",
   "metadata": {},
   "source": [
    "### Discordance\n",
    "\n",
    "The discordance matrix is a matrix that is used to represent the degree of discordance between pairs of alternatives. It is typically constructed by comparing the values of each alternative on each criterion, and determining whether the difference between the values is significant enough to cause discordance. \n",
    "\n",
    "The objective is to calculate the discordance between each pair of alternative and reference profiles and in both ways: \n",
    "- The discordance $D_j(a_i,b_k)$\n",
    "- The discordance $D_j(b_k,a_i)$ <br>\n",
    "*for $i$ the scenarios, $k$ the reference profiles and $j$ the criteria*\n",
    "\n",
    "Here is how the two types of discordance are calculated in the function: <br>\n",
    "<center>\n",
    "\n",
    "$D_j(a_i,b_k) = uj(b_k)-u_j(a_i)-p_j/v_j-p_j$<br>\n",
    "$D_j(b_k,a_i) = uj(a_i)-u_j(b_k)-p_j/v_j-p_j$<br>\n",
    "\n",
    "</center>\n",
    "\n",
    "\n",
    "If the value is higher than one it is replaced by one, and if it is smaller dans zero it is replaced by zero. \n",
    "\n",
    "\n",
    "Finally, the function returns two DataFrames : \n",
    "- `ddiscoa` : The discordance between the performances and the reference profiles $D_j(a_i,b_k)$\n",
    "- `ddiscob` : The discordance between the reference profiles and the performances $D_j(b_k,ba_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414dda50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def disco2(d):\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df2 = pd.DataFrame()\n",
    "    for column in d.iloc[:, 0:28]:\n",
    "        for col in d.iloc[:,30:36]:\n",
    "            alpha = (d[col]-d[column]-d[d.columns[37]])/(d[d.columns[38]]-d[d.columns[37]])\n",
    "            beta = (d[column]-d[col]-d[d.columns[37]])/(d[d.columns[38]]-d[d.columns[37]])\n",
    "            new_df[column+col+' a,b'] = alpha\n",
    "            new_df2[column+col+' b,a'] = beta\n",
    "    new_df[new_df<0]=0\n",
    "    new_df[new_df>1]=1\n",
    "    new_df2[new_df<0]=0\n",
    "    new_df2[new_df>1]=1\n",
    "    return new_df, new_df2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dfcf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disco(d):\n",
    "    temp = np.zeros(((16,168)))\n",
    "    new_df = pd.DataFrame(temp)\n",
    "    new_df2 = pd.DataFrame(temp)\n",
    "    q = d[d.columns[36]]\n",
    "    p = d[d.columns[37]]\n",
    "    v = d[d.columns[38]]\n",
    "    j=0\n",
    "    for sc in d.iloc[:, 0:28]: #pour chaque scénario sc\n",
    "        dscenar = d[sc]\n",
    "        i=0\n",
    "        for pr in d.iloc[:, 30:35]: #pour chaque profils\n",
    "            alpha = (dscenar-d[pr]+p)/(v-p)\n",
    "            beta = (d[pr]-dscenar+p)/(v-p)\n",
    "            pos = j*6+i\n",
    "            for c in new_df.index:\n",
    "                new_df[pos][c] = alpha[alpha.index[c]]\n",
    "                new_df2[pos][c] = beta[beta.index[c]]\n",
    "            i = i+1\n",
    "        j = j+1 \n",
    "    new_df[new_df<0]=0\n",
    "    new_df[new_df>1]=1\n",
    "    new_df2[new_df2<0]=0\n",
    "    new_df2[new_df2>1]=1\n",
    "    return new_df, new_df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e896ce2d",
   "metadata": {},
   "source": [
    "### Global concordance\n",
    "\n",
    "The function allows to calculate the global concordance of each scenario regarding each threshold. It takes as input the concordance matrix and the weights for each criteria. <br>\n",
    "*explain what si the global concordance for*\n",
    "\n",
    "The function takes as input the weights of each criterion, located in the `d` DataFrame as well as the concordance matrix, separated into 2 DataFrames previously : `dconca` and `dconcb`. \n",
    "\n",
    "The objective is, for each scenario calculate the following global concordance : \n",
    "\n",
    "<center>\n",
    "\n",
    "$C(a_i,b_k) = \\frac {\\sum_{j} C_j(a_i,b_k) * w_j}{\\sum_{j} w_j}$\n",
    "\n",
    "</center>\n",
    "\n",
    "*with i the scenarios and k the reference profiles*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aab7fb1f",
   "metadata": {},
   "source": [
    "autre version optimisée je sais pas encore laquelle choisir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_conc(d,dconc1):\n",
    "    new_df = pd.DataFrame(index=['b0', 'b1', 'b2', 'b3', 'b4', 'b5'], columns=['S1.1','S1.2','S1.3','S1.4','S2.1','S2.2','S2.3','S2.4','S3.1','S3.2','S3.3','S3.4','S4.1','S4.2','S4.3','S4.4','S5.1','S5.2','S5.3','S5.4','S6.1','S6.2','S6.3','S6.4','S7.1','S7.2','S7.3','S7.4']) \n",
    "    cols=dconc1.columns\n",
    "    i = 0\n",
    "    for j in range(0, len(cols),6):\n",
    "        a = sum(dconc1[cols[j]]*d[d.columns[28]])/sum(d[d.columns[28]]) \n",
    "        b = sum(dconc1[cols[j+1]]*d[d.columns[28]])/sum(d[d.columns[28]])  \n",
    "        c = sum(dconc1[cols[j+2]]*d[d.columns[28]])/sum(d[d.columns[28]]) \n",
    "        dr = sum(dconc1[cols[j+3]]*d[d.columns[28]])/sum(d[d.columns[28]])  \n",
    "        e = sum(dconc1[cols[j+4]]*d[d.columns[28]])/sum(d[d.columns[28]]) \n",
    "        f = sum(dconc1[cols[j+5]]*d[d.columns[28]])/sum(d[d.columns[28]]) \n",
    "        th = [a,b,c,dr,e,f]\n",
    "        new_df[new_df.columns[i]]= th\n",
    "        i = i+1\n",
    "    return new_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85cec1c2",
   "metadata": {},
   "source": [
    "### Degree of credibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f31262",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d2f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def credibility(dgconc, ddisc):\n",
    "    dcred = dgconc.copy()\n",
    "    cols = ddisc.columns\n",
    "    for j in range(0, len(cols),6):           #pour toutes les colonnes de discordance,toutes les 6 colonnes donc pour chaque scénario\n",
    "        ddiscj = [ddisc[ddisc.columns[j]], ddisc[ddisc.columns[j+1]], ddisc[ddisc.columns[j+2]], ddisc[ddisc.columns[j+3]], ddisc[ddisc.columns[j+4]], ddisc[ddisc.columns[j+5]]].copy()\n",
    "        cglobal = [dgconc[dgconc.columns[j/6]][0], dgconc[dgconc.columns[j/6]][1],dgconc[dgconc.columns[j/6]][2], dgconc[dgconc.columns[j/6]][3], dgconc[dgconc.columns[j/6]][4], dgconc[dgconc.columns[j/6]][5]]\n",
    "        dc = [0, 0, 0, 0, 0, 0]\n",
    "        for i in range(len(cglobal)):        #pour chaque profil de référence\n",
    "            verif = 0\n",
    "            for c in ddisc.index:           #parcours les valeurs de la colonne\n",
    "                if  ddisc[cols[j+i]][c] > cglobal[i]:           #si une valeur de la colonne est supérieur au coef de concordance global \n",
    "                    verif = verif + 1\n",
    "            if verif == 0 :\n",
    "                dc[i] = cglobal[i]\n",
    "            else: \n",
    "                df_mask = ddiscj[i]>cglobal[i] \n",
    "                filtered_ddisc = ddisc[df_mask]\n",
    "                degree = (((1-filtered_ddisc[cols[j+i]])/(1-cglobal[i])).prod())*cglobal[i] #le degré de credibilité du profil i et du scénario j\n",
    "        dcred[dcred.columns[j/6]] = dc\n",
    "    return dcred\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c8d87cf",
   "metadata": {},
   "source": [
    "Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_ranking_relations(creda, credb, λ):\n",
    "    new_df = creda.copy()\n",
    "    classementa = creda.apply(lambda x: x-λ)\n",
    "    classementb = credb.apply(lambda x: x-λ)\n",
    "    classementa[classementa>0]= 1 #surclasse\n",
    "    classementa[classementa<0]= 0 #ne surclasse pas\n",
    "    classementb[classementb>0]= 1\n",
    "    classementb[classementb<0]= 0\n",
    "    for i in creda:\n",
    "        for j in creda.index:\n",
    "            if classementa[i][j] == classementb[i][j] == 1: #si les 2 surclassent\n",
    "                new_df[i][j] = 'I'\n",
    "            elif classementa[i][j] == classementb[i][j] == 0: #si les 2 ne surclassement pas\n",
    "                new_df[i][j] = 'R'\n",
    "            elif classementa[i][j] == 0: #si seulement b surclasse s\n",
    "                new_df[i][j] = '<'\n",
    "            elif classementa[i][j] == 1: #si seulement s surclasse b\n",
    "                new_df[i][j] = '>'\n",
    "    return new_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07cc33b8",
   "metadata": {},
   "source": [
    "Pessimiste sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c83fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pessimistic_sort(dov,new_df):\n",
    "    # transpose the DataFrame so that the rows become columns\n",
    "    #stop_rows = []  # store the index of the row where the loop stopped for each column\n",
    "    cat = new_df.index\n",
    "    for col in dov: #pour le scéénario col  \n",
    "        etape = new_df[col] \n",
    "        print(col)\n",
    "        for j in reversed(range(len(dov.index))): \n",
    "            if dov[col][j] == '>' or dov[col][j] == 'I':\n",
    "                etape[etape.index[j]] = etape[etape.index[j]] +1\n",
    "                #new_df[col][new_df.index[j]] = new_df[col][new_df.index[j]]+1\n",
    "                break\n",
    "        new_df[col] = etape\n",
    "        print(new_df[col])\n",
    "    return new_df #, stop_rows\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4da92d51",
   "metadata": {},
   "source": [
    "Optimistic sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de8a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimistic_sort(dov,new_df):\n",
    "    # transpose the DataFrame so that the rows become columns\n",
    "    #stop_rows = []  # store the index of the row where the loop stopped for each colum\n",
    "    cat = new_df.index\n",
    "    for col in dov: \n",
    "        etape = new_df[col] \n",
    "        for j in range(len(dov.index)): \n",
    "            if dov[col][j] == '<' or dov[col][j] == 'R':\n",
    "                etape[etape.index[j-1]] = etape[etape.index[j-1]] +1\n",
    "                break\n",
    "        new_df[col] = etape \n",
    "        print(new_df[col])\n",
    "    return new_df #, stop_rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd90570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def electre_tri (d,rep):\n",
    "    temp = np.zeros((5,28))\n",
    "    pessi_sort = pd.DataFrame(temp, index=['C1', 'C2', 'C3', 'C4', 'C5'], columns=['S1.1','S1.2','S1.3','S1.4','S2.1','S2.2','S2.3','S2.4','S3.1','S3.2','S3.3','S3.4','S4.1','S4.2','S4.3','S4.4','S5.1','S5.2','S5.3','S5.4','S6.1','S6.2','S6.3','S6.4','S7.1','S7.2','S7.3','S7.4'])\n",
    "    opti_sort = pd.DataFrame(temp, index=['C1', 'C2', 'C3', 'C4', 'C5'], columns=['S1.1','S1.2','S1.3','S1.4','S2.1','S2.2','S2.3','S2.4','S3.1','S3.2','S3.3','S3.4','S4.1','S4.2','S4.3','S4.4','S5.1','S5.2','S5.3','S5.4','S6.1','S6.2','S6.3','S6.4','S7.1','S7.2','S7.3','S7.4'])\n",
    "    for i in range(rep) :\n",
    "        print(i)\n",
    "        d = MCarlo(d)\n",
    "        dconca, dconcb = conce2(d)\n",
    "        ddisca, ddiscb = disco2(d)\n",
    "        dgconca = global_conc(d,dconca)\n",
    "        dgconcb = global_conc(d,dconcb)\n",
    "        dcreda = credibility(dgconca, ddisca)\n",
    "        dcredb = credibility(dgconcb, ddiscb)\n",
    "        dranking = over_ranking_relations(dcreda, dcredb, λ)\n",
    "        print(dranking)\n",
    "        pessi_sort = pessimistic_sort(dranking,pessi_sort)\n",
    "        opti_sort = optimistic_sort(dranking,opti_sort)\n",
    "        print(pessi_sort)\n",
    "        print(opti_sort)\n",
    "    return opti_sort, pessi_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def electre_tri2 (d):\n",
    "    temp = np.zeros((5,28))\n",
    "    pessi_sort = pd.DataFrame(temp, index=['C1', 'C2', 'C3', 'C4', 'C5'], columns=['S1.1','S1.2','S1.3','S1.4','S2.1','S2.2','S2.3','S2.4','S3.1','S3.2','S3.3','S3.4','S4.1','S4.2','S4.3','S4.4','S5.1','S5.2','S5.3','S5.4','S6.1','S6.2','S6.3','S6.4','S7.1','S7.2','S7.3','S7.4'])\n",
    "    opti_sort = pd.DataFrame(temp, index=['C1', 'C2', 'C3', 'C4', 'C5'], columns=['S1.1','S1.2','S1.3','S1.4','S2.1','S2.2','S2.3','S2.4','S3.1','S3.2','S3.3','S3.4','S4.1','S4.2','S4.3','S4.4','S5.1','S5.2','S5.3','S5.4','S6.1','S6.2','S6.3','S6.4','S7.1','S7.2','S7.3','S7.4'])\n",
    "    d = MCarlo(d)\n",
    "    dconca, dconcb = conce2(d)\n",
    "    ddisca, ddiscb = disco(d)\n",
    "    dgconca = global_conc(d,dconca)\n",
    "    dgconcb = global_conc(d,dconcb)\n",
    "    dcreda = credibility(dgconca, ddisca)\n",
    "    dcredb = credibility(dgconcb, ddiscb)\n",
    "    dranking = over_ranking_relations(dcreda, dcredb, λ)\n",
    "    pessi_sort = pessimistic_sort(dranking,pessi_sort)\n",
    "    print(pessi_sort)\n",
    "    opti_sort = optimistic_sort(dranking,opti_sort)\n",
    "    print(opti_sort)\n",
    "    return opti_sort, pessi_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f57cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "repet = 1\n",
    "opti_sort, pessi_sort = electre_tri (d, repet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c7b84",
   "metadata": {},
   "source": [
    "Ici je pense on peut garder car prends directement les infos des csv mais prendre avec panda et pas faire \"append\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
